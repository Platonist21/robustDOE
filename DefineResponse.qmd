---
title: "Define the Response Variables of Robustness DOE"
author: "Charles Tan"
date: today
format:
  html:  
    embed-resources: true
editor: visual
---

## Notations

For an assay robustness DOE:

-   Let $N$ denote total number of runs, say 24
-   Let $n$ denote total number of center runs, say 8
-   Let $i=0$ index the center point condition
-   Let $i=1,2,â€¦,N-n$ index the non center point conditions
-   Let $X_i$ denote the row for condition $i$ in the design matrix
-   Let $K$ donate total number of samples, say 14 or 22
-   Let $T_{i,k}$ denote the observed log titer or log concentration of $k$th sample under $i$th condition
-   Let $\mu_{k}$ denote the mean log titer or log concentration of $k$th under center condition, $\mu_k=E(T_{0,k})$

## Primary Response

At each non center point condition, $T_{i,k}-\mu_k$ is the deviation from center point. Define $Y_i=E(T_{i,k}-\mu_k)^2$ where the expectation is over all samples $k$. This $Y_i$ is on the same scale as variance on the log scale, which has a one-to-one translation to %RSD scale. We can build a model $Y \sim X$ using SVEM to predict $Y_i$ given condition $X_i$, then impose a criterion on the $Y$ surface on the %RSD scale. If the whole $Y$ surface is below the criterion, we can conclude the assay is robust within the range of the design of the experiment.

The definition of $Y_i$ involves $K$ unknown (nuisance) parameters $\mu_{k}$. The simplest estimator of $\mu_{k}$ is $m_k=\frac{1}{n} \sum T_{0,k}$, then the simple naive estimator of $Y_i$ is $\frac{1}{K} \sum_{k}{(T_{i,k}-m_k)^2}$.

## Secondary Response

If the predicted $Y$ in some part of the design space is above the acceptance criterion, the natural followup question is why. The logical place to start the investigation is to look at bias induced by the conditions. Define $Z_i=E(T_{i,k}-\mu_k)$, again, the expectation is over all samples $k$. It can serve as the secondary response variable useful for investigation.

Plugging in $m_k$, we have the simple naive estimator of $Z_i$ as $\frac{1}{K} \sum_{k}{(T_{i,k}-m_k)}$.

## Key Idea

The key idea is that, instead of using the observations of $T_{i,k}$ directly as the response variable, we propose to use the second-order moment $Y_i$ and first-order moment $Z_i$, as the response variables for the analysis of assay robustness. Here, the distributional assumption is that, at given condition $i$, the samples have the same bias and variability. This assumption is pertinent because assay robustness is an assay property, not sample property. The sample set we happen to use is meant to represent all samples. Note that this assumption allows the bias and variability to be different at different condition $i$'s.

There is an implication of this assumption for the design of the experiment: at each condition $i$, more samples are more informative than more replications of the same samples.

At given condition $i$, there is a relationship between $Y_i$ and $Z_i$: $Y_i=Z_i^2+Var(T_{i,.})$. This relationship allows us to breakdown the reasons for $Y$ to be large: bias or random measurement variability or both.

## Model Averaging

At each condition $i>0$, there are two different scenarios:

The first scenario is that all the differences of $T_{i,k}-\mu_k$ are just measurement variability, i.e., $T_{i,k}-\mu_k=\epsilon_{i,k}$. Under this model, the maximum likelihood estimator for $Y_i$ is $Y_i^*=\frac{1}{K} \sum_{k}{(T_{i,k}-m_k)^2}$. (We can define $Z_i^*=0$.)

The second scenario is that there are both a constant bias and measurement variability in $T_{i,k}-\mu_k$, i.e., $T_{i,k}-\mu_k=\delta_i+\epsilon_{i,k}$. Under this model, the maximum likelihood estimators for the moments are as $Z_i^{**}=\frac{1}{K} \sum_{k}{(T_{i,k}-m_k)}$ and $Y_i^{**}=Z_i^{**2}+\frac{1}{K-1} \sum_{k}{(T_{i,k}-m_k-Z_i^{**})^2}$.

The two naive estimators are from two different scenarios / models, hence, are not congruent with each other.

Furthermore, we don't have to "pick a model". We can do "model averaging" by weighting the two sets of ML estimators according to their Akaike weights, i.e.,

$\hat{Y_i}=Aw_i^* \cdot Y_i^*+Aw_i^{**} \cdot Y_i^{**}$ and $\hat{Z_i}=Aw_i^* \cdot Z_i^*+Aw_i^{**} \cdot Z_i^{**}$.

Akaike weights should be obtained from $AIC_c$ of both models since $K$ is usually small to moderate. For now, let's assume, at condition $i>0$, we have one reportable value for each of the $K$ sample.

The first model corresponds to $N(0, \sigma^2)$, hence, $AIC_c^*=K \cdot log(2\pi) + K \cdot log(Y_i^*) +K+\frac{2K}{K-2}$.

The second model corresponds to $N(\mu, \sigma^2)$, hence, $AIC_c^{**}=K \cdot log(2\pi) + K \cdot log(Y_i^{**}) +K+\frac{4K}{K-3}$.

Let $A_{min}$ denote $\min(AIC_c^* , AIC_c^{**})$, define $\Delta AIC_c^*=AIC_c^* - A_{min}$ and $\Delta AIC_c^{**}=AIC_c^{**} - A_{min}$. Then $Aw_i^*=\frac{\exp(-0.5\cdot \Delta AIC_c^*)}{\exp(-0.5\cdot \Delta AIC_c^*)+\exp(-0.5\cdot \Delta AIC_c^{**})}$ and $Aw_i^{**}=\frac{\exp(-0.5\cdot \Delta AIC_c^{**})}{\exp(-0.5\cdot \Delta AIC_c^*)+\exp(-0.5\cdot \Delta AIC_c^{**})}$.

## At Center Point

For the analysis by SVEM, we can define $\hat{Z_0}=0$ and $\hat{Y_0}=\frac{1}{K} \sum_k{(T_{0,k}-m_k)^2}$.
